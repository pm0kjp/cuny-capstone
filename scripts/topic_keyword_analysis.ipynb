{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic / Keyword Analysis\n",
    "\n",
    "We want to get an idea what our blog is about, in order to match blogs from autistic bloggers with similar blogs from our \"control\" blogs.  We will conduct Term Frequency / Inverse Document Frequency analysis as well as use IBM Watson to detect concepts.  We work at the blog level, with all blog posts from the same blog concatenated into a single text.  This is because post-level matching would be prohibitively difficult, and it's sufficient to grasp the overall themes of a blog for matching purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, gensim\n",
    "from gensim.models import TfidfModel\n",
    "import json\n",
    "import sys\n",
    "import string\n",
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "from watson_developer_cloud.natural_language_understanding_v1 \\\n",
    "  import Features, ConceptsOptions, EntitiesOptions, KeywordsOptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a Word Count Tool\n",
    "\n",
    "We don't want to work with blogs that have fewer than 5000 words total, so we can skip analyzing these!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWC(fname):\n",
    "    num_words = 0\n",
    "    with open(fname, 'r') as f:\n",
    "        for line in f:\n",
    "            words = line.split()\n",
    "            num_words += len(words)\n",
    "    return(num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up for TF/IDF\n",
    "\n",
    "Much of the TF/IDF work in this section has been shamelessly borrowed from http://carrefax.com/new-blog/2017/11/25/create-a-gensim-corpus-for-text-files-in-a-local-directory ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_files(top_directory):\n",
    "    texts = []\n",
    "    for root, dirs, files in os.walk(top_directory):\n",
    "        for file in filter(lambda file: (file.endswith('.txt') and \\\n",
    "                                         getWC(os.path.join(root, file)) >= 5000), files):\n",
    "            texts.append(file)\n",
    "    return(texts)\n",
    "        \n",
    "def iter_documents(top_directory):\n",
    "    \"\"\"Iterate over all documents, yielding a document (=list of utf8 tokens) at a time.\"\"\"\n",
    "    for root, dirs, files in os.walk(top_directory):\n",
    "        for file in filter(lambda file: (file.endswith('.txt') and \\\n",
    "                                         getWC(os.path.join(root, file)) >= 5000), files):\n",
    "                document = open(os.path.join(root, file)).read() # read the entire document, as one big string\n",
    "                yield gensim.utils.tokenize(document, lower=True) # or whatever tokenization suits you\n",
    "\n",
    "class MyCorpus(object):\n",
    "    def __init__(self, top_dir):\n",
    "        self.top_dir = top_dir\n",
    "        self.dictionary = gensim.corpora.Dictionary(iter_documents(top_dir))\n",
    "        self.dictionary.filter_extremes(no_below=1, keep_n=30000) # check API docs for pruning params\n",
    "\n",
    "    def __iter__(self):\n",
    "        for tokens in iter_documents(self.top_dir):\n",
    "            yield self.dictionary.doc2bow(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getKeyWords(path) :\n",
    "    blogs_keyword_list = []\n",
    "    for blog in tfidf[list(MyCorpus(path))]:\n",
    "        sorted_tfidf_weights = sorted(blog, key=lambda w: w[1], reverse=True)\n",
    "        keywords = \"\"\n",
    "        for term_id, weight in sorted_tfidf_weights[:40]:\n",
    "            keywords = keywords + corpus.dictionary.get(term_id) + \", \"\n",
    "        blogs_keyword_list.append(keywords)\n",
    "    return(blogs_keyword_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform TF/IDF Analysis\n",
    "\n",
    "We want to analyze TF/IDF by group -- ASD in one section, controls in another.  This is so we don't detect very common themes in the blogs written by autistic bloggers -- terms like \"autism\" itself, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "asd_docs = [x.replace(\".txt\", \"\") for x in list_files(\"../confidential/corpora/consolidated_texts/ASD\")]\n",
    "controls_docs = [x.replace(\".txt\", \"\") for x in list_files(\"../confidential/corpora/consolidated_texts/controls\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "asd_kw = getKeyWords('../confidential/corpora/consolidated_texts/ASD')\n",
    "controls_kw = getKeyWords('../confidential/corpora/consolidated_texts/controls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Selected Blogs, Get Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd_wc = [getWC(os.path.join(\"../confidential/corpora/consolidated_texts/ASD\", (x+\".txt\"))) \\\n",
    "                for x in asd_docs]\n",
    "\n",
    "controls_wc = [getWC(os.path.join(\"../confidential/corpora/consolidated_texts/controls\", (x+\".txt\"))) \\\n",
    "               for x in controls_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Using IBM Watson to detect concepts\n",
    "\n",
    "We can only send text in 5000 character chunks, so we chunk each text into 5k characters, observing word boundaries, and send it to Watson.  When we get the data back on each chunk we can consolidate common topics.  It's important to save the Watson response, even if we don't want to use all of it, since there are limits to the number of free Watson requests we can make.\n",
    "\n",
    "It's easy and free to get credentials, see more at https://www.ibm.com/watson/services/natural-language-understanding/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "credentials = json.load(open('../confidential/watson_credentials.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we request a given version of the Watson NLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "  username=credentials[\"username\"],\n",
    "  password=credentials[\"password\"],\n",
    "  version='2018-03-16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case Watson is bogged down, can't analyze a text, etc., fail gracefully!  We will ask Watson to do hundreds of analyses at one go, so we'd hate to break just because of a handful of errors.  That's why we use try/except here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def watsonAnalysis(blog_text):\n",
    "    try:\n",
    "        response = natural_language_understanding.analyze(\n",
    "          text=blog_text,\n",
    "          features=Features(\n",
    "            entities=EntitiesOptions(\n",
    "              limit=15),\n",
    "            keywords=KeywordsOptions(\n",
    "              limit=15),\n",
    "            concepts=ConceptsOptions(\n",
    "              limit=15)))\n",
    "    except: \n",
    "        print (\"ERROR in Watson analysis with exception \" + str(sys.exc_info()[0]))\n",
    "        return(\"\")\n",
    "    return(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def segmentBlog(path):\n",
    "    blog_text = open(path,'r').read().replace('\\n', ' ')\n",
    "    import textwrap\n",
    "    bite_sized = textwrap.wrap(blog_text, 5000)\n",
    "    return(bite_sized)\n",
    "\n",
    "def blogSegmentsWatson(path):\n",
    "    bite_sized = segmentBlog(path)\n",
    "    watson = []\n",
    "    for bite in bite_sized[:15]:  \n",
    "        # if we have 300k words don't do all of them, just limit to first 75000 characters.\n",
    "        # that should do a good job of giving us analysis, no matter what\n",
    "        watson.append(watsonAnalysis(bite))\n",
    "    return(watson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "asd_blog_root = \"../confidential/corpora/consolidated_texts/ASD/\"\n",
    "controls_blog_root = \"../confidential/corpora/consolidated_texts/controls/\"\n",
    "asd_watson_list = []\n",
    "controls_watson_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's unsurprising to get some errors here.  We'll have enough 5000 character chunks on our blog posts to allow for some requests to fail..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR in Watson analysis with exception <class 'watson_developer_cloud.watson_service.WatsonApiException'>\n",
      "ERROR in Watson analysis with exception <class 'watson_developer_cloud.watson_service.WatsonApiException'>\n",
      "ERROR in Watson analysis with exception <class 'watson_developer_cloud.watson_service.WatsonApiException'>\n",
      "ERROR in Watson analysis with exception <class 'watson_developer_cloud.watson_service.WatsonApiException'>\n"
     ]
    }
   ],
   "source": [
    "for blog in asd_docs:\n",
    "    path = asd_blog_root + blog + \".txt\"\n",
    "    features = blogSegmentsWatson(path)\n",
    "    asd_watson_list.append(features)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR in Watson analysis with exception <class 'watson_developer_cloud.watson_service.WatsonApiException'>\n",
      "ERROR in Watson analysis with exception <class 'watson_developer_cloud.watson_service.WatsonApiException'>\n",
      "ERROR in Watson analysis with exception <class 'watson_developer_cloud.watson_service.WatsonApiException'>\n",
      "ERROR in Watson analysis with exception <class 'watson_developer_cloud.watson_service.WatsonApiException'>\n",
      "ERROR in Watson analysis with exception <class 'watson_developer_cloud.watson_service.WatsonApiException'>\n",
      "ERROR in Watson analysis with exception <class 'watson_developer_cloud.watson_service.WatsonApiException'>\n",
      "ERROR in Watson analysis with exception <class 'watson_developer_cloud.watson_service.WatsonApiException'>\n",
      "ERROR in Watson analysis with exception <class 'watson_developer_cloud.watson_service.WatsonApiException'>\n"
     ]
    }
   ],
   "source": [
    "for blog in controls_docs:\n",
    "    path = controls_blog_root + blog + \".txt\"\n",
    "    features = blogSegmentsWatson(path)\n",
    "    controls_watson_list.append(features)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "asd_watson_data = pd.DataFrame(asd_watson_list)\n",
    "controls_watson_data = pd.DataFrame(controls_watson_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "asd_watson_data.to_csv(\"../confidential/watson_asd.csv\")\n",
    "controls_watson_data.to_csv(\"../confidential/watson_controls.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "asd_entities = []\n",
    "asd_keywords = []\n",
    "asd_concepts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for blog in asd_watson_list:\n",
    "    entities = []\n",
    "    keywords = []\n",
    "    concepts = []\n",
    "    for chunk in blog:\n",
    "        if len(chunk) > 0 :\n",
    "            for keyword in chunk.get(\"keywords\"):\n",
    "                if keyword.get(\"relevance\") > 0.85:\n",
    "                    keywords.append(keyword.get(\"text\"))\n",
    "            for entity in chunk.get(\"entities\"):\n",
    "                if entity.get(\"relevance\") > 0.85:\n",
    "                    entities.append(entity.get(\"text\"))\n",
    "            for concept in chunk.get(\"concepts\"):\n",
    "                if concept.get(\"relevance\") > 0.85:\n",
    "                    concepts.append(concept.get(\"text\"))\n",
    "    entities = set(entities)\n",
    "    keywords = set(keywords)\n",
    "    concepts = set(concepts)\n",
    "    asd_entities.append(entities)\n",
    "    asd_keywords.append(keywords)\n",
    "    asd_concepts.append(concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "controls_entities = []\n",
    "controls_keywords = []\n",
    "controls_concepts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for blog in controls_watson_list:\n",
    "    entities = []\n",
    "    keywords = []\n",
    "    concepts = []\n",
    "    for chunk in blog:\n",
    "        if len(chunk) > 0 :\n",
    "            for keyword in chunk.get(\"keywords\"):\n",
    "                if keyword.get(\"relevance\") > 0.85:\n",
    "                    keywords.append(keyword.get(\"text\"))\n",
    "            for entity in chunk.get(\"entities\"):\n",
    "                if entity.get(\"relevance\") > 0.85:\n",
    "                    entities.append(entity.get(\"text\"))\n",
    "            if chunk.get(\"concepts\"): # some don't have concepts\n",
    "                for concept in chunk.get(\"concepts\"):\n",
    "                    if concept.get(\"relevance\") > 0.85:\n",
    "                        concepts.append(concept.get(\"text\"))\n",
    "            else:\n",
    "                concepts.append(\"\")\n",
    "    entities = set(entities)\n",
    "    keywords = set(keywords)\n",
    "    concepts = set(concepts)\n",
    "    controls_entities.append(entities)\n",
    "    controls_keywords.append(keywords)\n",
    "    controls_concepts.append(concepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine What We Know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "asd_blog_features = pd.DataFrame({'blog' : asd_docs,\n",
    "                                  'word_count' : asd_wc,\n",
    "                                  'tf_idf_keywords' : asd_kw,\n",
    "                                  'keywords' : asd_keywords, \n",
    "                                  'entities' : asd_entities,\n",
    "                                  'concepts': asd_concepts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "controls_blog_features = pd.DataFrame({'blog' : controls_docs,\n",
    "                                       'word_count' : controls_wc,\n",
    "                                       'tf_idf_keywords' : controls_kw,\n",
    "                                       'keywords' : controls_keywords, \n",
    "                                       'entities' : controls_entities,\n",
    "                                       'concepts': controls_concepts})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Matching Data to File\n",
    "\n",
    "We want to review the matching data qualitatively -- it takes too much human knowledge to match similar blogs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "asd_blog_features.to_csv(\"../confidential/asd_matching.csv\")\n",
    "controls_blog_features.to_csv(\"../confidential/controls_matching.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
