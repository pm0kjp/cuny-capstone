{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blog Scraping\n",
    "\n",
    "## Part 1: Setup\n",
    "\n",
    "First, we load modules we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from scipy.stats.mstats import zscore\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from scipy.stats.mstats import zscore\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "%matplotlib inline  \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping requires us to be respectful of the intellectual property of and server impact to the owners of the site we're scraping.\n",
    "\n",
    "We will crawl sites based on known sitemap architectures. We do not attempt a full-fledged spidering of sites and do not follow links.\n",
    "\n",
    "## Part 2: Define Functions\n",
    "\n",
    "The getHTML function simply gets an entire webpage.  This is platform independent (will work for Blogspot, Wordpress, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHTML(url):\n",
    "    try:\n",
    "        f = urllib.urlopen(url)\n",
    "    except: \n",
    "        print (\"ERROR in \" + url + \" with exception \" + str(sys.exc_info()[0]))\n",
    "        return(404)\n",
    "    if f.getcode() != 200:\n",
    "        return(404)\n",
    "    htmltext = f.read()\n",
    "    return(htmltext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many bloggers use the Wordpress platform for blogging.  Wordpress has a certain way it structures its html as well as blog entries, so we have a pair of functions that handle both the parsing of an individual post as well as the spidering of the blog (looking for all posts in a given time frame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseWordpressSite(htmltext):\n",
    "    soup = BeautifulSoup(htmltext, \"lxml\")\n",
    "\n",
    "    # remove any .feedback (they're within our .post divs so we want them out so that \"Comments\", e.g., won't be included)\n",
    "    for div in soup.find_all(\"p\", {'class':'feedback'}): \n",
    "        div.decompose()\n",
    "\n",
    "    # remove any .storydate (they're within our .post divs so we want them out, so that \"March\", e.g., won't be included)\n",
    "    for div in soup.find_all(class_ = \"storydate\"): \n",
    "        div.decompose()\n",
    "\n",
    "    # get only .post divs and .entry-content divs.\n",
    "    posthtml = soup.find_all(\"div\", class_=\"post\") + soup.find_all(\"div\", class_=\"entry-content\")\n",
    "    posttext = \"\"\n",
    "    for post in posthtml:\n",
    "        posttext += post.getText()\n",
    "    return(posttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrapeWordpressBlog (blog):\n",
    "        year=2018\n",
    "        month=2\n",
    "        blogstring = \"\"\n",
    "        # recursively construct a single text that combines all blog text from this blogger from 2011-present\n",
    "        while year > 2010:\n",
    "            page = site.strip() + str(\"%04d\" % (year)) + \"/\" + str(\"%02d\" % (month))\n",
    "            if month==1 :\n",
    "                month = 12\n",
    "                year = year - 1\n",
    "            else:\n",
    "                month = month - 1\n",
    "            htmltext = getHTML(page)\n",
    "            if htmltext==404:  \n",
    "                continue     # go to the next month.\n",
    "            poststring = parseWordpressSite(htmltext).encode(\"utf8\")\n",
    "            blogstring = blogstring + \" \" + poststring\n",
    "        # remove and replace smart quotes, unreadable characters, new line chars, etc.\n",
    "        blogstring = blogstring.replace(\"\\xe2\\x80\\x9c\", \"'\").replace(\"\\xe2\\x80\\x9d\", \"'\") \n",
    "        blogstring = blogstring.replace('\\xe2\\x80\\x92', \" \").replace('\\xe2\\x80\\x93', \" \").replace('\\xe2\\x80\\x94', \" \")\n",
    "        blogstring = blogstring.replace(\"\\xe2\\x80\\x98\", \"'\").replace('\\xe2\\x80\\x99', \"'\")                                                                                                  \n",
    "        blogstring = blogstring.replace('\\n', \" \").replace('\\t', \" \").replace('\\xc2\\xa0',\" \")\n",
    "        blogstring = blogstring.replace(\"\\'\", \"'\")\n",
    "        return blogstring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Obtain list of blogs\n",
    "\n",
    "Note that the actual blogs used are confidential, to preserve the privacy of the bloggers.  We pull here from a .csv that has the main url of the blog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autismBlogList = pd.read_table(\"ASD_wordpress_bloggers.txt\", squeeze=True).tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
