{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blog Scraping\n",
    "\n",
    "The goal of this script is to perform a number of tasks for each blog:\n",
    "\n",
    "- Spider the site to find all in-site url references to the site's blog posts (using regex to define what a blog text -- as opposed to a listing of posts or index page, etc. --  looks like)\n",
    "- Get all unique blog text urls\n",
    "- Go to each url and extract the text, adding it to a corpus of the blog texts for that blog\n",
    "- Save corpora to disk, so that we only have to crawl once and can analyze over and over without hitting the server\n",
    "\n",
    "After we set things up, we will demonstrate on a sample blog (so that you can see how the blog list should be set up) and also run the blog scraper on our (confidential) list of subject blogs.\n",
    "\n",
    "\n",
    "## Part 1: Setup\n",
    "\n",
    "First, we load modules we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also important to indicate what our assumptions are as far as data that this script needs.  \n",
    "\n",
    "This script depends on a blog list, which will be a data frame with two columns:\n",
    "* blog identifier (something like \"1\", \"TEST-1\", \"Autism-1\", etc.)\n",
    "* blog url, ending in `/` (e.g. \"https://my.fake.blog.org/\")\n",
    "    \n",
    "This script will also write files to disk, so if you run it, you will need to run it in a directory that has a subdirectory called \"confidential\" and a directory under that called \"corpora\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping requires us to be respectful of the intellectual property of and server impact to the owners of the site we're scraping.\n",
    "\n",
    "We will crawl sites based on known sitemap architectures. We do not attempt a full-fledged spidering of sites and do not follow links.\n",
    "\n",
    "## Part 2: Define Functions\n",
    "\n",
    "The `getHTML` function simply gets an entire webpage.  This is platform independent (will work for Blogspot, Wordpress, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getHTML(url):\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "    except: \n",
    "        print (\"ERROR in \" + url + \" with exception \" + str(sys.exc_info()[0]))\n",
    "        return(404)\n",
    "    if r.status_code != 200:\n",
    "        return(404)\n",
    "    htmltext = r.text\n",
    "    return(htmltext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`getWordpressLinks` takes a base url (like \"http://my.fake.blog.org/\") as a parameter, and uses what we know about Wordpress sites (blog posts are located in the `[site]/year/month/day` path, and links to blog posts for a given month can be located at `[site]/year/month`) to find all relevant links to blog posts for a given time point.  Links are then vetted according to the following rules:  \n",
    "\n",
    "* They must have the form `[site]/year/month/day/[post]`\n",
    "* They must not be double-counted -- we will strip duplicates using `set()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWordpressLinks (site):\n",
    "        pattern = re.compile(re.escape(site) + \"\\d{4}/\\d{2}/\\d{2}/.+\")\n",
    "        links = set() # makes entries unique\n",
    "        year=2018 # stop point\n",
    "        month=3 # stop point\n",
    "        # recursively construct a single text that combines all blog text from this blogger from 2010-present\n",
    "        while year >= 2018:\n",
    "            page = site.strip() + str(\"%04d\" % (year)) + \"/\" + str(\"%02d\" % (month))\n",
    "            if month==1 :\n",
    "                month = 12\n",
    "                year = year - 1\n",
    "            else:\n",
    "                month = month - 1\n",
    "            html_content = getHTML(page)\n",
    "            if html_content!=404:\n",
    "                soup = BeautifulSoup(html_content, 'lxml')\n",
    "                for a in soup.find_all('a', href=True):\n",
    "                    possible_link = a.get('href')\n",
    "                    m = re.match(pattern, possible_link)\n",
    "                    if m:\n",
    "                        links.add(possible_link)\n",
    "        return(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `getLinks` function will take a blog list (see above for the expected format) and return a list of every link that looks like a blog post for each blog in the blog list. It uses the `getWordpressLinks` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLinks(blogList):\n",
    "    links = pd.DataFrame(columns=[\"blog_identifier\",\"links\"])\n",
    "    for index, site in blogList.iterrows(): \n",
    "        new = pd.DataFrame({\"blog_identifier\": site[0], \"links\": list(getWordpressLinks(site[1]))})\n",
    "        links = pd.concat([links, new] )\n",
    "    return(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `groupLinks` function takes a data frame that has many rows, each of which has a single blog post and the blog identifier it came from, and returns a shorter data frame, where each blog identifier has a single row in which there is a list of all blog posts from that blog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def groupLinks(blogList):\n",
    "    grouped = getLinks(blogList).groupby(\"blog_identifier\")[\"links\"].apply(lambda x: list(x))\n",
    "    return(grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many bloggers use the Wordpress platform for blogging.  Wordpress has a certain way it structures its html as well as blog entries, so we have a pair of functions that handle both the parsing of an individual post as well as the spidering of the blog (looking for all posts in a given time frame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseWordpressSite(htmltext):\n",
    "    soup = BeautifulSoup(htmltext, \"lxml\")\n",
    "    \n",
    "    # remove any .feedback (they're within our .post divs so we want them out so that \"Comments\", e.g., won't be included)\n",
    "    for div in soup.find_all(\"p\", {'class':'feedback'}): \n",
    "        div.decompose()\n",
    "\n",
    "    for script in soup.find_all (\"script\"):\n",
    "        script.decompose()\n",
    "        \n",
    "    # remove ads\n",
    "    for div in soup.find_all(\"div\", {'class': 'wpa'}):\n",
    "        div.decompose()\n",
    "        \n",
    "    # remove sharing links\n",
    "    for div in soup.find_all(\"div\", {'class': 'sharedaddy'}):\n",
    "        div.decompose()    \n",
    "    \n",
    "    # remove any .storydate (they're within our .post divs so we want them out, so that \"March\", e.g., won't be included)\n",
    "    for div in soup.find_all(class_ = \"storydate\"): \n",
    "        div.decompose()\n",
    "\n",
    "    # get only .post divs and .entry-content divs.\n",
    "    posthtml = soup.find_all(\"div\", class_=\"post\") + soup.find_all(\"div\", class_=\"entry-content\")\n",
    "    posttext = \"\"\n",
    "    for post in posthtml:\n",
    "        posttext += post.getText()\n",
    "    return(posttext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`writeCorporaToDisk` takes a grouped list of blog posts, scrapes the entire list of blog posts, and saves the scraped text and saves it as a file with the name including the blog identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeCorporaToDisk(blogList):\n",
    "    for blog_identifier, post_list in groupLinks(blogList).iteritems():\n",
    "        print(\"analyzing blog \" + str(blog_identifier))\n",
    "        blogstring = \"\"\n",
    "        for blog_url in post_list:\n",
    "            htmltext = getHTML(blog_url)\n",
    "            poststring = parseWordpressSite(htmltext)\n",
    "            blogstring = blogstring + \" \" + poststring\n",
    "        # remove and replace smart quotes, unreadable characters, new line chars, etc.\n",
    "        blogstring = blogstring.replace(\"\\xe2\\x80\\x9c\", \"'\").replace(\"\\xe2\\x80\\x9d\", \"'\") \n",
    "        blogstring = blogstring.replace('\\xe2\\x80\\x92', \" \").replace('\\xe2\\x80\\x93', \" \").replace('\\xe2\\x80\\x94', \" \")\n",
    "        blogstring = blogstring.replace(\"\\xe2\\x80\\x98\", \"'\").replace('\\xe2\\x80\\x99', \"'\")                                                                                                  \n",
    "        blogstring = blogstring.replace('\\n', \" \").replace('\\t', \" \").replace('\\xc2\\xa0',\" \")\n",
    "        blogstring = blogstring.replace(\"\\'\", \"'\")\n",
    "        # Write this blog's total corpus to file\n",
    "        file_name = \"../confidential/corpora/blog_\" + str(blog_identifier) + \".txt\"\n",
    "        text_file = open(file_name, \"w\")\n",
    "        text_file.write(blogstring)\n",
    "        text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Obtain list of blogs\n",
    "\n",
    "Note that the actual blogs used are confidential, to preserve the privacy of the bloggers.  We pull here from a text file that has the main url of each blog on a separate line.  Note that to preserve the privacy of subjects, the blog lists are not included in the GitHub repository for this project.  This will *not* work for you, unless you create your own `ASD_wordpress_bloggers.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autismWordpressBlogList = pd.read_csv(\"../confidential/ASD_wordpress_bloggers.csv\", header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Obtain blog texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing blog 1\n",
      "analyzing blog 2\n"
     ]
    }
   ],
   "source": [
    "writeCorporaToDisk(autismWordpressBlogList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Demonstrate How This Works\n",
    "\n",
    "We are keeping the blogs themselves secret, so how can you know that this works?\n",
    "\n",
    "You will want to make sure you run this notebook in a directory that has a subdirectory called \"confidential\" which in turn contains \"corpora\".  Then, you can do the following.  NOTE that the blog I selected as examples is *not* a blog that is actually used in this research.  It is just a sample wordpress blog, intended to demonstrate how this notebook works and that this script works effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_blogs = pd.DataFrame(data = {\"blog_identifier\":['TEST1'], \n",
    "                            \"blog_url\":[\"https://en.blog.wordpress.com/\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blog_identifier</th>\n",
       "      <th>blog_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST1</td>\n",
       "      <td>https://en.blog.wordpress.com/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  blog_identifier                        blog_url\n",
       "0           TEST1  https://en.blog.wordpress.com/"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleLinks = getLinks(sample_blogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blog_identifier</th>\n",
       "      <th>links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST1</td>\n",
       "      <td>https://en.blog.wordpress.com/2018/02/02/reade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST1</td>\n",
       "      <td>https://en.blog.wordpress.com/2018/01/03/the-w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST1</td>\n",
       "      <td>https://en.blog.wordpress.com/2018/02/14/manag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  blog_identifier                                              links\n",
       "0           TEST1  https://en.blog.wordpress.com/2018/02/02/reade...\n",
       "1           TEST1  https://en.blog.wordpress.com/2018/01/03/the-w...\n",
       "2           TEST1  https://en.blog.wordpress.com/2018/02/14/manag..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleGrouped = groupLinks(sample_blogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "blog_identifier\n",
       "TEST1    [https://en.blog.wordpress.com/2018/02/02/read...\n",
       "Name: links, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleGrouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing blog TEST1\n"
     ]
    }
   ],
   "source": [
    "writeCorporaToDisk(sample_blogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, check in your confidential/corpora directory and you should see a new text file that contains all the scraped text in a single text file!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
